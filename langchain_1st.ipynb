{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJzyRC9ZlSnMtxj2lqzeXj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aicentaurs/Flowise/blob/main/langchain_1st.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "M2ZLTceDILVl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eeda414-beba-44aa-cbf6-f6c14edd5189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.176)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.7)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
            "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
            "Requirement already satisfied: typing-inspect>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d69hPeAJePk_",
        "outputId": "4036db97-be70-44ab-edeb-0cd418d41daf"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.7)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export OPENAI_API_KEY=\"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\""
      ],
      "metadata": {
        "id": "8pfU4qi5eYDX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "Z6QmId8QDhJE"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "LEQQoKRgDtH8"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "B40F87TaDu8W"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "3-2Ga2rAD3jW"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(product=\"colorful socks\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szBr-YtZEBlz",
        "outputId": "b458d8c8-89eb-4d08-8f05-a7886e17e759"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is a good name for a company that makes colorful socks?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFVirsAmD1sn",
        "outputId": "bf647ddd-4904-4752-e067-3192559fddbf"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Rainbow Toes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"What is a good name for a company that makes {product}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "WdFuLvrBEKp4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "nvMhEmhpEWsp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"colorful socks\")\n",
        "# -> '\\n\\nSocktastic!'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "t9R7kpL7EY0m",
        "outputId": "7fdc1169-4107-4db0-f1f7-ee3b35ac3cc4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nColorful Cozy Socks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\""
      ],
      "metadata": {
        "id": "tCEdeHWPElhQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"What was the high temperature in MLPS yesterday in Fahrenheit? What is that number raised to the .023 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "7WNPGtIkEsC8",
        "outputId": "3cd1e0e9-ca34-46b6-88b2-4ff3e617d0b2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find the temperature first, then use the calculator to raise it to the .023 power.\n",
            "Action: Search\n",
            "Action Input: \"High temperature in MLPS yesterday in Fahrenheit\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHigh & Low Weather Summary for the Past Weeks ; 78 째F (May 10, 6:53 pm), 97% (May 6, 6:53 am), 30.48 \"Hg (May 6, 6:53 am) ; 46 째F (May 19, 6:53 am), 32% (May 15, ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find the temperature from the observation\n",
            "Action: Search\n",
            "Action Input: \"High temperature in MLPS yesterday in Fahrenheit\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mHigh & Low Weather Summary for the Past Weeks ; 78 째F (May 10, 6:53 pm), 97% (May 6, 6:53 am), 30.48 \"Hg (May 6, 6:53 am) ; 46 째F (May 19, 6:53 am), 32% (May 15, ...\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now have the temperature, I need to use the calculator to raise it to the .023 power\n",
            "Action: Calculator\n",
            "Action Input: 78^.023\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.1053967308913755\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: 1.1053967308913755\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.1053967308913755'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-search-results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ojeej8Qpsr1g",
        "outputId": "479c676e-00a7-47ea-b5ce-ee209a82d616"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"]=\"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\""
      ],
      "metadata": {
        "id": "CuWXDBN7BxgF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "output = conversation.predict(input=\"Hi there!\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MOGk_6NF_gG",
        "outputId": "bcfbda9a-33be-4fc5-d8b5-7edf7638e6a9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " Hi there! It's nice to meet you. How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJmjw5XwGRch",
        "outputId": "a3214da8-c86f-4135-8cdf-39675fc00215"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hi there! It's nice to meet you. How can I help you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "tWRs5N72Gv7s"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "    HumanMessage(content=\"I love programming.\")\n",
        "]\n",
        "chat(messages)\n",
        "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGEsdMdQGyMd",
        "outputId": "7772e407-1543-4c37-ac14-b4c92a1a82a8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%history -n\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enwK2jLxIKPF",
        "outputId": "75eac7bd-aa2d-4dcc-f4cf-d7f02a93bdc2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1: !ls\n",
            "   2: !pip install langchain\n",
            "   3: pip install openai\n",
            "   4: !pip install openai\n",
            "   5: !export OPENAI_API_KEY=\"...\"\n",
            "   6: pip list\n",
            "   7: pip install google-search-results\n",
            "   8: pip check\n",
            "   9: pip check\n",
            "  10: pip list\n",
            "  11:\n",
            "if \"SERPAPI_API_KEY\" in os.environ:\n",
            "    print(\"SERPAPI_API_KEY is set\")\n",
            "else:\n",
            "    print(\"SERPAPI_API_KEY is not set\")\n",
            "  12:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]= 92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\n",
            "  13:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]=92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\n",
            "  14:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]=\"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\"\n",
            "  15:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]=\"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\"\n",
            "  16:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  17: !export OPENAI_API_KEY=\"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\"\n",
            "  18:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  19:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  20:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]=\"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\"\n",
            "  21: !pip install langchain\n",
            "  22: !pip install openai\n",
            "  23: !export OPENAI_API_KEY=\"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\"\n",
            "  24: from langchain.llms import OpenAI\n",
            "  25: llm = OpenAI(temperature=0.9)\n",
            "  26: export OPENAI_API_KEY=\"...\"\n",
            "  27: !export OPENAI_API_KEY=\"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\"\n",
            "  28: from langchain.llms import OpenAI\n",
            "  29: llm = OpenAI(temperature=0.9)\n",
            "  30:\n",
            "import os\n",
            "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
            "  31: from langchain.llms import OpenAI\n",
            "  32: from langchain.llms import OpenAI\n",
            "  33: llm = OpenAI(temperature=0.9)\n",
            "  34: llm = OpenAI(temperature=0.9)\n",
            "  35:\n",
            "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
            "print(llm(text))\n",
            "  36:\n",
            "from langchain.prompts import PromptTemplate\n",
            "\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"product\"],\n",
            "    template=\"What is a good name for a company that makes {product}?\",\n",
            ")\n",
            "  37:\n",
            "from langchain.prompts import PromptTemplate\n",
            "\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"product\"],\n",
            "    template=\"What is a good name for a company that makes {product}?\",\n",
            ")\n",
            "  38: print(prompt.format(product=\"colorful socks\"))\n",
            "  39:\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "llm = OpenAI(temperature=0.9)\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"product\"],\n",
            "    template=\"What is a good name for a company that makes {product}?\",\n",
            ")\n",
            "  40:\n",
            "from langchain.chains import LLMChain\n",
            "chain = LLMChain(llm=llm, prompt=prompt)\n",
            "  41:\n",
            "from langchain.chains import LLMChain\n",
            "chain = LLMChain(llm=llm, prompt=prompt)\n",
            "  42:\n",
            "chain.run(\"colorful socks\")\n",
            "# -> '\\n\\nSocktastic!'\n",
            "  43:\n",
            "chain.run(\"colorful socks\")\n",
            "# -> '\\n\\nSocktastic!'\n",
            "  44:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"] = \"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\"\n",
            "  45:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  46: pip install google-search-results\n",
            "  47:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  48:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in MLPS yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  49:\n",
            "from langchain import OpenAI, ConversationChain\n",
            "\n",
            "llm = OpenAI(temperature=0)\n",
            "conversation = ConversationChain(llm=llm, verbose=True)\n",
            "\n",
            "output = conversation.predict(input=\"Hi there!\")\n",
            "print(output)\n",
            "  50:\n",
            "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
            "print(output)\n",
            "  51:\n",
            "from langchain.chat_models import ChatOpenAI\n",
            "from langchain.schema import (\n",
            "    AIMessage,\n",
            "    HumanMessage,\n",
            "    SystemMessage\n",
            ")\n",
            "\n",
            "chat = ChatOpenAI(temperature=0)\n",
            "  52:\n",
            "messages = [\n",
            "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
            "    HumanMessage(content=\"I love programming.\")\n",
            "]\n",
            "chat(messages)\n",
            "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n",
            "  53: %history -n\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
        "        HumanMessage(content=\"I love artificial intelligence.\")\n",
        "    ],\n",
        "]\n",
        "result = chat.generate(batch_messages)\n",
        "result\n",
        "# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV588C4wJzmN",
        "outputId": "5e3a7ae0-502a-4644-8382-f7b97d7a6999"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(text=\"J'adore la programmation.\", generation_info=None, message=AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False))], [ChatGeneration(text=\"J'adore l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'adore l'intelligence artificielle.\", additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}, 'model_name': 'gpt-3.5-turbo'})"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.llm_output['token_usage']\n",
        "# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_iamRYQJlMA",
        "outputId": "e80fe744-f4d1-421c-d0e5-29efded37e1b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"German\", text=\"I love programming.\").to_messages())\n",
        "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHBm_vOJKAjf",
        "outputId": "81677771-82a0-428b-e712-ddeced04c511"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ich liebe Programmieren.', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
        "# -> \"J'aime programmer.\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0-HeBuViK1fd",
        "outputId": "ddac74a0-a6b9-483c-ef52-35174b837ce9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"J'adore la programmation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "fNNgMWOsLbP9",
        "outputId": "651d13a2-a1b0-4918-c574-98bb6cb2c3d7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: I need to use a search engine to find Olivia Wilde's boyfriend and a calculator to raise his age to the 0.23 power.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Olivia Wilde boyfriend\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mSudeikis and Wilde's relationship ended in November 2020. Wilde was publicly served with court documents regarding child custody while she was presenting Don't Worry Darling at CinemaCon 2022. In January 2021, Wilde began dating singer Harry Styles after meeting during the filming of Don't Worry Darling.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI need to use a search engine to find Harry Styles' current age.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Harry Styles age\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m29 years\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow I need to calculate 29 raised to the 0.23 power.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"29^0.23\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2.169459462491557\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.\n",
            "Final Answer: 2.169459462491557\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.169459462491557'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate, \n",
        "    MessagesPlaceholder, \n",
        "    SystemMessagePromptTemplate, \n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
        "\n",
        "conversation.predict(input=\"tell me how to maximize my langchain agent's powers and tools.\")\n",
        "# -> 'Hello! How can I assist you today?'\n",
        "\n",
        "\n",
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
        "# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\n",
        "\n",
        "conversation.predict(input=\"Tell me about yourself.\")\n",
        "# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "FUxtpwbqLu7p",
        "outputId": "cff56327-1296-40e0-b6a2-2485315d0598"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure! I am an AI language model designed to understand and respond to natural language queries. I was created using deep learning algorithms and trained on a large dataset of text and speech data. My goal is to provide helpful and informative responses to your questions and to assist you in any way I can. I am constantly learning and improving, so feel free to ask me anything you'd like!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%history -n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUUpMglUhc1H",
        "outputId": "e256e07c-9ef8-4f77-fbf9-6f793904d1dc"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   1: !ls\n",
            "   2: !pip install langchain\n",
            "   3: pip install openai\n",
            "   4: !pip install openai\n",
            "   5: !export OPENAI_API_KEY=\"...\"\n",
            "   6: pip list\n",
            "   7: pip install google-search-results\n",
            "   8: pip check\n",
            "   9: pip check\n",
            "  10: pip list\n",
            "  11:\n",
            "if \"SERPAPI_API_KEY\" in os.environ:\n",
            "    print(\"SERPAPI_API_KEY is set\")\n",
            "else:\n",
            "    print(\"SERPAPI_API_KEY is not set\")\n",
            "  12:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]= 92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\n",
            "  13:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]=92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\n",
            "  14:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]=\"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\"\n",
            "  15:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]=\"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\"\n",
            "  16:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  17: !export OPENAI_API_KEY=\"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\"\n",
            "  18:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  19:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  20:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"]=\"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\"\n",
            "  21: !pip install langchain\n",
            "  22: !pip install openai\n",
            "  23: !export OPENAI_API_KEY=\"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\"\n",
            "  24: from langchain.llms import OpenAI\n",
            "  25: llm = OpenAI(temperature=0.9)\n",
            "  26: export OPENAI_API_KEY=\"...\"\n",
            "  27: !export OPENAI_API_KEY=\"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\"\n",
            "  28: from langchain.llms import OpenAI\n",
            "  29: llm = OpenAI(temperature=0.9)\n",
            "  30:\n",
            "import os\n",
            "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
            "  31: from langchain.llms import OpenAI\n",
            "  32: from langchain.llms import OpenAI\n",
            "  33: llm = OpenAI(temperature=0.9)\n",
            "  34: llm = OpenAI(temperature=0.9)\n",
            "  35:\n",
            "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
            "print(llm(text))\n",
            "  36:\n",
            "from langchain.prompts import PromptTemplate\n",
            "\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"product\"],\n",
            "    template=\"What is a good name for a company that makes {product}?\",\n",
            ")\n",
            "  37:\n",
            "from langchain.prompts import PromptTemplate\n",
            "\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"product\"],\n",
            "    template=\"What is a good name for a company that makes {product}?\",\n",
            ")\n",
            "  38: print(prompt.format(product=\"colorful socks\"))\n",
            "  39:\n",
            "from langchain.prompts import PromptTemplate\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "llm = OpenAI(temperature=0.9)\n",
            "prompt = PromptTemplate(\n",
            "    input_variables=[\"product\"],\n",
            "    template=\"What is a good name for a company that makes {product}?\",\n",
            ")\n",
            "  40:\n",
            "from langchain.chains import LLMChain\n",
            "chain = LLMChain(llm=llm, prompt=prompt)\n",
            "  41:\n",
            "from langchain.chains import LLMChain\n",
            "chain = LLMChain(llm=llm, prompt=prompt)\n",
            "  42:\n",
            "chain.run(\"colorful socks\")\n",
            "# -> '\\n\\nSocktastic!'\n",
            "  43:\n",
            "chain.run(\"colorful socks\")\n",
            "# -> '\\n\\nSocktastic!'\n",
            "  44:\n",
            "import os\n",
            "os.environ[\"SERPAPI_API_KEY\"] = \"92dc50ed0e35fa0862ae86f27fbd4c166159638bd37e03a88b59497b3ff73ed8\"\n",
            "  45:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  46: pip install google-search-results\n",
            "  47:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  48:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "llm = OpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"What was the high temperature in MLPS yesterday in Fahrenheit? What is that number raised to the .023 power?\")\n",
            "  49:\n",
            "from langchain import OpenAI, ConversationChain\n",
            "\n",
            "llm = OpenAI(temperature=0)\n",
            "conversation = ConversationChain(llm=llm, verbose=True)\n",
            "\n",
            "output = conversation.predict(input=\"Hi there!\")\n",
            "print(output)\n",
            "  50:\n",
            "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
            "print(output)\n",
            "  51:\n",
            "from langchain.chat_models import ChatOpenAI\n",
            "from langchain.schema import (\n",
            "    AIMessage,\n",
            "    HumanMessage,\n",
            "    SystemMessage\n",
            ")\n",
            "\n",
            "chat = ChatOpenAI(temperature=0)\n",
            "  52:\n",
            "messages = [\n",
            "    SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
            "    HumanMessage(content=\"I love programming.\")\n",
            "]\n",
            "chat(messages)\n",
            "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n",
            "  53: %history -n\n",
            "  54:\n",
            "result.llm_output['token_usage']\n",
            "# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}\n",
            "  55:\n",
            "batch_messages = [\n",
            "    [\n",
            "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
            "        HumanMessage(content=\"I love programming.\")\n",
            "    ],\n",
            "    [\n",
            "        SystemMessage(content=\"You are a helpful assistant that translates English to French.\"),\n",
            "        HumanMessage(content=\"I love artificial intelligence.\")\n",
            "    ],\n",
            "]\n",
            "result = chat.generate(batch_messages)\n",
            "result\n",
            "# -> LLMResult(generations=[[ChatGeneration(text=\"J'aime programmer.\", generation_info=None, message=AIMessage(content=\"J'aime programmer.\", additional_kwargs={}))], [ChatGeneration(text=\"J'aime l'intelligence artificielle.\", generation_info=None, message=AIMessage(content=\"J'aime l'intelligence artificielle.\", additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}})\n",
            "  56:\n",
            "result.llm_output['token_usage']\n",
            "# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}\n",
            "  57:\n",
            "from langchain.chat_models import ChatOpenAI\n",
            "from langchain.prompts.chat import (\n",
            "    ChatPromptTemplate,\n",
            "    SystemMessagePromptTemplate,\n",
            "    HumanMessagePromptTemplate,\n",
            ")\n",
            "\n",
            "chat = ChatOpenAI(temperature=0)\n",
            "\n",
            "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
            "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
            "human_template = \"{text}\"\n",
            "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
            "\n",
            "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
            "\n",
            "# get a chat completion from the formatted messages\n",
            "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"French\", text=\"I love programming.\").to_messages())\n",
            "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n",
            "  58:\n",
            "from langchain.chat_models import ChatOpenAI\n",
            "from langchain.prompts.chat import (\n",
            "    ChatPromptTemplate,\n",
            "    SystemMessagePromptTemplate,\n",
            "    HumanMessagePromptTemplate,\n",
            ")\n",
            "\n",
            "chat = ChatOpenAI(temperature=0)\n",
            "\n",
            "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
            "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
            "human_template = \"{text}\"\n",
            "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
            "\n",
            "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
            "\n",
            "# get a chat completion from the formatted messages\n",
            "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"German\", text=\"I love programming.\").to_messages())\n",
            "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})\n",
            "  59:\n",
            "from langchain.chat_models import ChatOpenAI\n",
            "from langchain import LLMChain\n",
            "from langchain.prompts.chat import (\n",
            "    ChatPromptTemplate,\n",
            "    SystemMessagePromptTemplate,\n",
            "    HumanMessagePromptTemplate,\n",
            ")\n",
            "\n",
            "chat = ChatOpenAI(temperature=0)\n",
            "\n",
            "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
            "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
            "human_template = \"{text}\"\n",
            "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
            "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
            "\n",
            "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
            "chain.run(input_language=\"English\", output_language=\"French\", text=\"I love programming.\")\n",
            "# -> \"J'aime programmer.\"\n",
            "  60:\n",
            "from langchain.agents import load_tools\n",
            "from langchain.agents import initialize_agent\n",
            "from langchain.agents import AgentType\n",
            "from langchain.chat_models import ChatOpenAI\n",
            "from langchain.llms import OpenAI\n",
            "\n",
            "# First, let's load the language model we're going to use to control the agent.\n",
            "chat = ChatOpenAI(temperature=0)\n",
            "\n",
            "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
            "llm = OpenAI(temperature=0)\n",
            "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
            "\n",
            "\n",
            "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
            "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
            "\n",
            "# Now let's test it out!\n",
            "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")\n",
            "  61:\n",
            "from langchain.prompts import (\n",
            "    ChatPromptTemplate, \n",
            "    MessagesPlaceholder, \n",
            "    SystemMessagePromptTemplate, \n",
            "    HumanMessagePromptTemplate\n",
            ")\n",
            "from langchain.chains import ConversationChain\n",
            "from langchain.chat_models import ChatOpenAI\n",
            "from langchain.memory import ConversationBufferMemory\n",
            "\n",
            "prompt = ChatPromptTemplate.from_messages([\n",
            "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
            "    MessagesPlaceholder(variable_name=\"history\"),\n",
            "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
            "])\n",
            "\n",
            "llm = ChatOpenAI(temperature=0)\n",
            "memory = ConversationBufferMemory(return_messages=True)\n",
            "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
            "\n",
            "conversation.predict(input=\"tell me how to maximize my langchain agent's powers and tools.\")\n",
            "# -> 'Hello! How can I assist you today?'\n",
            "\n",
            "\n",
            "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
            "# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\"\n",
            "\n",
            "conversation.predict(input=\"Tell me about yourself.\")\n",
            "# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\"\n",
            "  62: %history -n\n",
            "  63: %history -n\n"
          ]
        }
      ]
    }
  ]
}